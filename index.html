<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand Open Knowledge Benchmarking</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <div class="nav-brand">
                <h1 class="logo">OKBench</h1>
            </div>
            <nav class="nav">
                <button class="nav-btn active" data-tab="overview">Overview</button>
                <button class="nav-btn" data-tab="leaderboard">Leaderboard</button>
                <button class="nav-btn" data-tab="usage">Usage</button>
            </nav>
        </div>
    </header>

    <!-- Main Content -->
    <main class="main">
        <div class="container">
            <!-- Hero Section -->
            <section class="hero">
                <h1 class="hero-title">OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand Open Knowledge Benchmarking</h1>
                
                <div class="authors">
                    <p class="authors-text">
                        Anonymous Authors
                    </p>
                </div>

                <div class="action-buttons">
                    <a href="#" class="btn btn-primary">
                        <i class="fas fa-file-alt"></i>
                        Paper
                    </a>
                    <a href="https://github.com/<your-org>/kode.git" class="btn btn-secondary" target="_blank">
                        <i class="fab fa-github"></i>
                        Code
                    </a>
                    <a href="#" class="btn btn-secondary">
                        <i class="fas fa-database"></i>
                        Data
                    </a>
                    <a href="#leaderboard" class="btn btn-secondary" onclick="showTab('leaderboard')">
                        <i class="fas fa-trophy"></i>
                        Leaderboard
                    </a>
                </div>
            </section>

            <!-- Tab Content -->
            <div class="tab-content">
                <!-- Overview Tab -->
                <div id="overview" class="tab-panel active">
                    <section class="abstract">
                        <h2>Abstract</h2>
                        <p>Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements.</p>
                        <p>To address these drawbacks, we propose OpenKnowledgeBench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks.</p>
                        <p>Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on multiple open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge.</p>
                        <p>Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.</p>
                    </section>

                    <section class="introduction">
                        <h2>Introduction</h2>
                        <p>One of the most common uses of large language models (LLMs) is for answering knowledge-intensive questions. However, this task is challenging as factual knowledge in the real world evolves rapidly. Well-trained models can quickly become outdated, raising the need for continual model updates or improved retrieval-augmented generation (RAG) techniques.</p>
                        <p>At the same time, the lack of transparency in training data makes it difficult to assess how recent a model's knowledge truly is. Existing benchmarks also struggle to keep pace: once released, their contents may be absorbed into future training data, leading to benchmark saturation and weakening their utility.</p>
                        <p>We propose that the solution to these challenges is <strong>fast, automated, decentralized curation of dynamic knowledge benchmarks</strong> that can track LLM development in real time and offer a clean testbed for evaluating retrieval augmented methods.</p>
                        
                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Benchmark</th>
                                        <th>Human Involvement</th>
                                        <th>Automation</th>
                                        <th>Update Freq. & Scale</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>StreamingQA</td>
                                        <td>Partial (curated + synthetic)</td>
                                        <td>Partial</td>
                                        <td>Static</td>
                                    </tr>
                                    <tr>
                                        <td>RealTime QA</td>
                                        <td>Yes (media-sourced quizzes)</td>
                                        <td>Partial</td>
                                        <td>Weekly (~30 QA pairs)</td>
                                    </tr>
                                    <tr>
                                        <td>FreshQA</td>
                                        <td>Yes (human-written)</td>
                                        <td>Low</td>
                                        <td>Weekly (answers only)</td>
                                    </tr>
                                    <tr class="highlight">
                                        <td><strong>Ours (OKBench)</strong></td>
                                        <td><strong>No (auto-generated)</strong></td>
                                        <td><strong>Full</strong></td>
                                        <td><strong>Any time (~2000 QA pairs)</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>

                    <section class="automated-benchmarking">
                        <h2>Automated Dynamic Benchmarking with OKBench</h2>
                        
                        <div class="pipeline-section">
                            <h3>Benchmark Construction Pipeline</h3>
                            <div class="pipeline-image">
                                <img src="assets/pipeline_new_page-0001.jpg" alt="OKBench Pipeline" />
                            </div>
                            <p>We design an agentic framework for dynamic knowledge benchmarking. The pipeline consists of four steps: (1) News extraction, (2) QA generation, (3) question validation, and (4) dataset versioning.</p>
                            
                            <div class="pipeline-steps">
                                <div class="step">
                                    <h4>News Extraction</h4>
                                    <p>We collect and preprocess news articles published within the past 24 hours from a diverse set of outlets, including both mainstream and specialized publications. Articles are retrieved via RSS feeds and parsed.</p>
                                </div>
                                
                                <div class="step">
                                    <h4>QA Generation</h4>
                                    <p>We use an LLM-based agent to generate initial multiple-choice questionâ€“answer pairs from curated news articles. The agent is instructed to prioritize recent and unique facts, particularly entities, events, and developments that are unlikely to appear in older training data.</p>
                                </div>
                                
                                <div class="step">
                                    <h4>Question Validation</h4>
                                    <p>We introduce a dedicated question validation agent that assesses the quality and clarity of each question. The agent verifies whether each question can be answered uniquely and unambiguously, checking for direct references to source articles and ensuring clear date references.</p>
                                </div>
                                
                                <div class="step">
                                    <h4>Dataset Versioning</h4>
                                    <p>Each benchmark release is assigned a unique signature serving as its version identifier. Each signature encodes the agent LLM model name and version, decoding hyperparameters, dataset generation date and timestamp, and a randomly generated hash.</p>
                                </div>
                            </div>
                        </div>

                        <div class="news-sources">
                            <h3>News Sources</h3>
                            <div class="sources-grid">
                                <div class="source-category">
                                    <h4>General / Mainstream News</h4>
                                    <p>CNN, BBC, Reuters, The Guardian, Fox News, NBC News, USA Today, HuffPost, CBS News</p>
                                </div>
                                <div class="source-category">
                                    <h4>International Coverage</h4>
                                    <p>Al Jazeera, DW, RT, Channel News Asia (CNA), Times of India, South China Morning Post (SCMP)</p>
                                </div>
                                <div class="source-category">
                                    <h4>Technology and Science</h4>
                                    <p>TechCrunch, The Verge, Engadget, Ars Technica, Gizmodo, PC Gamer, TechRadar</p>
                                </div>
                                <div class="source-category">
                                    <h4>Business / Finance</h4>
                                    <p>Bloomberg</p>
                                </div>
                                <div class="source-category">
                                    <h4>Lifestyle / Culture</h4>
                                    <p>GQ, Vanity Fair</p>
                                </div>
                                <div class="source-category">
                                    <h4>Open-Source Community News</h4>
                                    <p>WikiNews</p>
                                </div>
                            </div>
                        </div>

                        <div class="example-qa">
                            <h3>Example Generated QA Pairs</h3>
                            <div class="qa-examples">
                                <div class="qa-example">
                                    <p><strong>Question:</strong> As of February 26, 2025, what percentage of GDP has UK Prime Minister Keir Starmer announced the country will spend on defense?</p>
                                    <div class="choices">
                                        <span>A. 2.3% of its GDP</span>
                                        <span>B. 3% of its GDP</span>
                                        <span>C. 2.5% of its GDP</span>
                                        <span>D. 7% of its GDP</span>
                                    </div>
                                    <p><strong>Answer:</strong> C. 2.5% of its GDP</p>
                                </div>
                                
                                <div class="qa-example">
                                    <p><strong>Question:</strong> On February 14, 2025, at which hospital was Pope Francis hospitalized for a respiratory infection?</p>
                                    <div class="choices">
                                        <span>A. St. Peter's Hospital</span>
                                        <span>B. Vatican Medical Center</span>
                                        <span>C. Gemelli Hospital</span>
                                        <span>D. Apostolic Palace Clinic</span>
                                    </div>
                                    <p><strong>Answer:</strong> C. Gemelli Hospital</p>
                                </div>
                            </div>
                        </div>
                    </section>

                    <section class="evaluation-results">
                        <h2>Evaluation Results</h2>
                        
                        <div class="results-summary">
                            <h3>Key Findings</h3>
                            <div class="findings">
                                <div class="finding">
                                    <h4>Impact of Fresh Knowledge</h4>
                                    <p>When models must rely solely on parametric memory (No context), their performance is far from perfect across all sizes. This reflects the challenge of truly new facts that arise after the model's pretraining cutoff.</p>
                                </div>
                                
                                <div class="finding">
                                    <h4>Oracle Context and Reading Comprehension</h4>
                                    <p>Once the ground-truth article is appended to the query, most models (above a certain size threshold) quickly climb to high accuracy (~95%). Even a 4-7B parameter model can answer correctly given the right passage.</p>
                                </div>
                                
                                <div class="finding">
                                    <h4>Model Size Scaling</h4>
                                    <p>The gap between smaller and larger models in the No context setting is smaller than one might expect from standard benchmarks that rely heavily on memorized knowledge. For fresh news QA, model scaling shows more limited benefits compared to memorized knowledge benchmarks.</p>
                                </div>
                            </div>
                        </div>

                        <div class="results-visualization">
                            <h3>Performance Visualization</h3>
                            <div class="images-grid">
                                <div class="image-container">
                                    <img src="assets/dynamic_qa_with_mmlu_reference_4_models_page-0001.jpg" alt="Dynamic QA Performance with MMLU Reference" />
                                    <p>No context vs. Oracle context QA Accuracy on OKBench, plotted alongside each model's performance on MMLU Pro as a reference for memorized knowledge.</p>
                                </div>
                                <div class="image-container">
                                    <img src="assets/no_context_and_oracle_page-0001.jpg" alt="No Context and Oracle Performance" />
                                    <p>Comparison of model performance with and without oracle context.</p>
                                </div>
                                <div class="image-container">
                                    <img src="assets/retrieval_accuracy_page-0001.jpg" alt="Retrieval Accuracy" />
                                    <p>Top-k Retrieval Accuracy for BM25, DPR, and ColBERT v2 across news corpora of different time windows.</p>
                                </div>
                            </div>
                        </div>
                    </section>
                </div>

                <!-- Leaderboard Tab -->
                <div id="leaderboard" class="tab-panel">
                    <section class="leaderboard-section">
                        <h2>Leaderboard</h2>
                        <p class="leaderboard-description">Performance results of various LLMs on OKBench across different evaluation dates.</p>
                        
                        <div class="leaderboard-filters">
                            <label for="date-select">Select Date:</label>
                            <select id="date-select">
                                <option value="2025-06-07">2025-06-07</option>
                                <option value="2025-03-22">2025-03-22</option>
                            </select>
                        </div>

                        <div class="leaderboard-table-container">
                            <table class="leaderboard-table" id="leaderboard-table">
                                <thead>
                                    <tr>
                                        <th>Rank</th>
                                        <th>Model</th>
                                        <th>No Context Accuracy</th>
                                        <th>Oracle Accuracy</th>
                                        <th>Date</th>
                                    </tr>
                                </thead>
                                <tbody id="leaderboard-body">
                                    <!-- Data will be populated by JavaScript -->
                                </tbody>
                            </table>
                        </div>
                    </section>
                </div>

                <!-- Usage Tab -->
                <div id="usage" class="tab-panel">
                    <section class="usage-section">
                        <h2>How to Use OKBench</h2>
                        <p class="usage-description">Generate your own dynamic knowledge benchmarks with OKBench. The framework is fully automated and can be run on-demand to create fresh evaluation datasets.</p>

                        <div class="installation">
                            <h3>ðŸš€ Quick-start</h3>
                            
                            <div class="step-section">
                                <h4>0 Â· Install</h4>
                                <div class="code-block">
                                    <pre><code>git clone https://github.com/<your-org>/kode.git
cd kode
conda env create -f environment.yml   # python 3.9-3.11
conda activate kode</code></pre>
                                </div>
                                <div class="tip">
                                    <strong>Tip:</strong> GPU strongly recommended for dense retrieval; CPU is fine for dataset generation.
                                </div>
                            </div>

                            <div class="step-section">
                                <h4>1 Â· Generate Dataset (Single Command)</h4>
                                <p>The unified <code>main.py</code> script handles everything automatically:</p>
                                
                                <h5>Basic Usage</h5>
                                <div class="code-block">
                                    <pre><code>python main.py --date 2025-06-07 --api_key YOUR_OPENAI_API_KEY</code></pre>
                                </div>

                                <h5>With LLM Judge Filtering</h5>
                                <div class="code-block">
                                    <pre><code>python main.py \
    --date 2025-06-07 \
    --api_key YOUR_OPENAI_API_KEY \
    --use_llm_judge \
    --llm_judge_model gpt-4.1-2025-04-14</code></pre>
                                </div>

                                <h5>Custom Model</h5>
                                <div class="code-block">
                                    <pre><code>python main.py \
    --date 2025-06-07 \
    --api_key YOUR_OPENAI_API_KEY \
    --model gpt-4o \
    --version 0</code></pre>
                                </div>
                            </div>
                        </div>

                        <div class="what-it-does">
                            <h3>What it does automatically:</h3>
                            <ul>
                                <li><strong>For today/yesterday:</strong> Fetches fresh news from RSS feeds</li>
                                <li><strong>For older dates:</strong> Downloads pre-extracted news from <a href="https://github.com/yanhong-lbh/kode-leaderboard/tree/main/public/news_snapshots" target="_blank">GitHub repository</a></li>
                                <li><strong>Generates dataset:</strong> Creates QA pairs using your specified model</li>
                                <li><strong>Optional filtering:</strong> Uses LLM judge to filter high-quality questions</li>
                            </ul>
                        </div>

                        <div class="arguments">
                            <h3>Arguments:</h3>
                            <ul>
                                <li><code>--date</code>: Date string in YYYY-MM-DD format (required)</li>
                                <li><code>--api_key</code>: Your OpenAI API key (required)</li>
                                <li><code>--model</code>: OpenAI model for QA generation (default: gpt-4.1-2025-04-14)</li>
                                <li><code>--version</code>: Prompt template version (default: 0)</li>
                                <li><code>--use_llm_judge</code>: Enable LLM judge filtering (optional)</li>
                                <li><code>--llm_judge_model</code>: Model for judge filtering (default: gpt-4.1-2025-04-14)</li>
                            </ul>
                        </div>

                        <div class="output-files">
                            <h3>Output files:</h3>
                            <ul>
                                <li><code>all_news/news_2025-06-07.json</code> - Raw news articles</li>
                                <li><code>2025-06-07_final_output_v0.json</code> - Raw generated QAs</li>
                                <li><code>2025-06-07_transformed_v0.json</code> - Post-processed dataset with signature</li>
                                <li><code>2025-06-07_filtered_by_llm_judge_v0.json</code> - Filtered dataset (if using judge)</li>
                            </ul>
                        </div>

                        <div class="github-link">
                            <h3>GitHub Repository</h3>
                            <p>Access the complete source code and contribute to OKBench:</p>
                            <a href="https://github.com/<your-org>/kode.git" class="btn btn-primary" target="_blank">
                                <i class="fab fa-github"></i>
                                View on GitHub
                            </a>
                        </div>
                    </section>
                </div>
            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 OKBench. All rights reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
